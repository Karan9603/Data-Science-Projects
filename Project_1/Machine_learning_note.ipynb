{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "## Machine learning (ML) notes ##\n",
    "\n",
    "# ML is subfield of AI\n",
    "# ML allows programs to learn to recognize patterens on its own and make predictions.\n",
    "# types of ML\n",
    "    #supervised learning (labelled)\n",
    "    #unsupervised learning (not labelled)\n",
    "#supervised learning which is labelled data:\n",
    "# if catagorical --> result come as cataogry (low, medium , high) (yes, no) --> # it is clasification problem #\n",
    "# if numerical --> give nummerical result  --> # it is regression problem #\n",
    "\n",
    "## supervised learning -- here we give data and learn the machine or prepare the machine. \n",
    "    So when we have new data machine checks and give us result based on we learned the machine before.\n",
    "\n",
    "## unsupervised learning -- he we give data but without label. so machine gives output themself by making groups what machine thinks.\n",
    "\n",
    "## semisupervised -- here we give mixed data to machine and guid little bit like which group we wat in output\n",
    "\n",
    "## reinforcement learning -- here we give condition to machine to process inside. \n",
    "\n",
    "### Steps for clasification problem\n",
    "    # collect data\n",
    "        Data Structuring - add , delete , merge coloumns\n",
    "    # prepare data - clean, organize, remove errors or missing values\n",
    "    # Explore Data - Use statistical and data visualization methods to help you discover relationships between the different features, \n",
    "        understand patterns, interpret the information, and achieve insights. It is during the EDA that you confirm or discard your hypothesis.\n",
    "    # Encoding categorical variables\n",
    "    # Normalisation --> MinMaxScaler \n",
    "    # standardisation\n",
    "        -- Use normalisation techniques when you know that the distribution of your data is skewed.\n",
    "        -- On the other hand, standardisation can be helpful in cases where the data follows a Gaussian distribution (normal distribution).\n",
    "        Also, outliers will not be affected by standardisation.\n",
    "        link - https://www.analyticsvidhya.com/blog/2020/04/feature-scaling-machine-learning-normalization-standardization/\n",
    "    # Chosse model - (eg. Naive Bayes, SVM)\n",
    "    # Train the model - its like giving the model a set of practice questions\n",
    "    # Evaluate the model - give new data and check results if work ok\n",
    "    # Parameter tuning and model improvement with k-Fold cross-validation\n",
    "    # Fine-tune Model - do some last correction if needed so that result will be more accurate\n",
    "    # make the predictions - get the last result and handover furher\n",
    "\n",
    "# Model (Algorithms) for classification\n",
    "        logistic regression\n",
    "        decision Trees\n",
    "        Random Forest\n",
    "        Support Vector Machines (SVM)\n",
    "        K-Nearest Neighbours (KNN)\n",
    "        Naive Bayes\n",
    "        Gradient Boosting\n",
    "        Neural Networks\n",
    "\n",
    "# Methods for Hyperparameter tuning\n",
    "    GridSerchCV --> here each data point check seperatly\n",
    "    RandomizedSearchCV --> here radom points (with given numbers) checked and it shows area where we have high\n",
    "\n",
    "    For Big data better to do RandomizedSearchCV to get area so that we can do only in that area GridSerchCV. \n",
    "    it reduces time for evaluation\n",
    "\n",
    "# Cross Validation\n",
    "\n",
    "    it is used when data is imbalanced data (not distributed same).\n",
    "    here we split data into parts (which we given) and run through that all prats and show which part is show best result.\n",
    "    \n",
    "    # strachied cross validatio\n",
    "    here we take same percentage from all cataogry. e.g like 10% from low, 10% from medium and 10% from high\n",
    "        \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# with catagory Low, medium and High\n",
    "R_wine = pd.read_csv(r'c:\\Data Analysis\\Wine Dataset\\winequality-red.csv', sep= ';') \n",
    "W_wine = pd.read_csv(r'c:\\Data Analysis\\Wine Dataset\\winequality-white.csv', sep= ';')\n",
    "R_wine['wine_type'] = 0\n",
    "W_wine['wine_type'] = 1\n",
    "\n",
    "R_wine ['quality label'] = pd.cut(R_wine['quality'],bins=[0,5,7,10],labels=['low','medium','high'],right=True)\n",
    "R_wine['quality label']=pd.Categorical(R_wine['quality label'], categories= ['low', 'medium', 'high'], ordered= True )\n",
    "\n",
    "W_wine ['quality label'] = pd.cut(W_wine['quality'],bins=[0,5,7,10],labels=['low','medium','high'],right=True)\n",
    "W_wine['quality label']=pd.Categorical(W_wine['quality label'], categories= ['low', 'medium', 'high'], ordered= True )\n",
    "\n",
    "RW_wine = pd.concat ((R_wine,W_wine), ignore_index=True)\n",
    "RW_wine.info()"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
